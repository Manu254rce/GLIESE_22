{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gliese_22\n",
    "\n",
    "A neural network project based on classifying Deep Space Objects(DSOs). The project is currently on the development stages, where there's still so much work to be done in terms of expanding the datasets involved and adding new functionalities for computationally intensive tasks.\n",
    "\n",
    "We now start by importing the required modules and libraries\n",
    "\n",
    "    1. Tensorflow - the library that will be used when building the Neural Network,including training and testing. Using Tensorflow, we have access to Keras(the top level API which we use to access other low level methods like layers, optimizers,loss functions and preprocessing via the ImageDataGenerator class))\n",
    "    2. Matplotlib - Python's MATLAB-like library that enables graphical representation of data and statistical analysis. It also contains libraries which can be used to view images as data plots by converting the pixel values into a multidimensional array and then plotting these as it were a graph.\n",
    "    3. Pandas - a library that can be used to perform data visualisation. Here, we use it in exploratory data analysis by obtaining data files and importing them into our code as dataframe objects. Using Pandas, we can also perform set operations on our data, sorting data and even defining our own dataframes\n",
    "    4. NumPy - a Python library that allows us to work with array datatypes in Python\n",
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, wget\n",
    " \n",
    "# from sklearn.model_selection import train_test_split\n",
    "from keras import layers, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.utils import img_to_array, load_img\n",
    "from keras.optimizers import RMSprop\n",
    "# from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importing pretrained models: Transfer Learning\n",
    "\n",
    "The Inception V3 model is one of the most sophisticated Neural Network models out there, with about 40 convolution layers deep, making it capable enough for any type of image classification, including space objects. Developed by Google in 2014, it allows one to use it as a base model from which similar models can be trained on different types of images. In this case, for example, the Inception V3 model, having being trained on 14 million images, can enable me to build a CNN that is trained to classify images of DSOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and define the InceptionV3 model\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3), include_top=False, weights=None)\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "\n",
    "We then import our datasets which will be used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stars = pd.read_csv('./data/Datasets/HYG_Catalogue.csv')\n",
    "\n",
    "data_ngcic = pd.read_excel('./data/Datasets/DSO-NGCIC Classification.xlsx', sheet_name='NGCIC Classification')\n",
    "data_dso = pd.read_excel('./data/Datasets/DSO-NGCIC Classification.xlsx', sheet_name='DSO Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can view details of our dataset, such as the columns, size of the data as well as how it has been structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Messier Objects:', data_dso.shape)\n",
    "print('NGCIC Objects:', data_ngcic.shape)\n",
    "print('Data shape: ', data_ngcic.shape, data_dso.shape)\n",
    "print('Details: ', data_ngcic.columns, data_dso.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "There are multiple datasets involved in this project. This means that there's a lot of data to deal with. So to better understand our data, we derive as much information from our data as possible. Exploratory data analysis involves the process of building relations between any two variables, extracting subsets of the data and detecting anomalies like null entries or inconsistent data in terms of datatypes, problems that may later lead to future problems like overfitting or underfitting.\n",
    "\n",
    "In the first analysis, we develop a night sky map where we use Right Ascension and Declination as the astronomical coordinate systems used in locating various objects in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the dataset\n",
    "\n",
    "data_ngcic['float ra'] = data_ngcic['ra hr'] + data_ngcic['ra min'] / 60 + data_ngcic['ra sec'] / 3600\n",
    "data_ngcic['float dec'] = data_ngcic['dec deg '] + data_ngcic['dec min'] / 60 + data_ngcic['dec sec'] / 3600\n",
    "\n",
    "# plot the data from various variables\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.ylim(-90, 90)\n",
    "plt.xlabel('Right Ascension')\n",
    "plt.ylabel('Declination')\n",
    "plt.xticks(np.arange(0, 24, 1), labels=['0h', '1h', '2h', '3h', '4h', '5h', '6h', '7h', '8h', '9h',\n",
    "                                        '10h', '11h', '12h', '13h', '14h', '15h', '16h', '17h', '18h',\n",
    "                                        '19h', '20h', '21h', '22h', '23h'])\n",
    "plt.yticks(np.arange(-90, 90, 10), labels=['-90deg', '-80deg', '-70deg', '-60deg', '-50deg', '-40deg', '-30deg',\n",
    "                                           '-20deg', '-10deg', '0deg', '10deg', '20deg', '30deg', '40deg', '50deg',\n",
    "                                           '60deg', '70deg', '80deg'])\n",
    "plt.title('Plot for NGCIC Objects in the Sky', fontsize=20, fontweight='bold')\n",
    "plt.scatter(data_ngcic['float ra'], data_ngcic['float dec'], s=0.09, c='red')\n",
    "plt.scatter(stars['ra'], stars['dec'], s=0.01, c='blue')\n",
    "plt.legend(['NGCIC Objects', 'Stars'], loc='upper right', fontsize=16, markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we establish a relationship between visible magnitude(brightness) of an object against it's distance from Earth, in metric units. From this, we can tell that some objects tend to follow a common trend, such as galaxies, nebulae and star clusters. Such can be used to develop clustering techniques in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the image data, we use matplotlib to define a 10x10 array of subplots, each with a random image from our './data/Images' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/Images'\n",
    "\n",
    "train_dir = os.path.join( base_dir, 'training_set')\n",
    "validation_dir = os.path.join( base_dir, 'validation_set')\n",
    "\n",
    "train_dark_nebula_dir = os.path.join(train_dir, 'dark_nebula') \n",
    "train_diffuse_nebula_dir = os.path.join(train_dir, 'diffuse_nebula') \n",
    "train_dwarf_elliptical_dir = os.path.join(train_dir, 'dwarf_elliptical') \n",
    "train_elliptical_galaxy_dir = os.path.join(train_dir, 'elliptical_galaxy') \n",
    "train_globular_cluster_dir = os.path.join(train_dir, 'globular_cluster') \n",
    "train_interacting_galaxy_dir = os.path.join(train_dir, 'interacting_galaxy') \n",
    "train_irregular_galaxy_dir = os.path.join(train_dir, 'irregular_galaxy') \n",
    "train_open_cluster_dir = os.path.join(train_dir, 'open_cluster') \n",
    "train_planetary_nebula_dir = os.path.join(train_dir, 'planetary_nebula') \n",
    "train_spiral_galaxy_dir = os.path.join(train_dir, 'spiral_galaxy') \n",
    "train_supernova_rem_dir = os.path.join(train_dir, 'supernova_rem') \n",
    "\n",
    "\n",
    "validation_dark_nebula_dir = os.path.join(validation_dir, 'dark_nebula') \n",
    "validation_diffuse_nebula_dir = os.path.join(validation_dir, 'diffuse_nebula') \n",
    "validation_dwarf_elliptical_dir = os.path.join(validation_dir, 'dwarf_elliptical') \n",
    "validation_elliptical_galaxy_dir = os.path.join(validation_dir, 'elliptical_galaxy') \n",
    "validation_globular_cluster_dir = os.path.join(validation_dir, 'globular_cluster') \n",
    "validation_interacting_galaxy_dir = os.path.join(validation_dir, 'interacting_galaxy') \n",
    "validation_irregular_galaxy_dir = os.path.join(validation_dir, 'irregular_galaxy') \n",
    "validation_open_cluster_dir = os.path.join(validation_dir, 'open_cluster') \n",
    "validation_planetary_nebula_dir = os.path.join(validation_dir, 'planetary_nebula') \n",
    "validation_spiral_galaxy_dir = os.path.join(validation_dir, 'spiral_galaxy') \n",
    "validation_supernova_rem_dir = os.path.join(validation_dir, 'supernova_rem') \n",
    "\n",
    "\n",
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size = 20,\n",
    "                                                    class_mode = 'categorical', \n",
    "                                                    target_size = (150, 150))     \n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
    "                                                          batch_size  = 20,\n",
    "                                                          class_mode  = 'categorical', \n",
    "                                                          target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dark_nebula_names = os.listdir(train_dark_nebula_dir)\n",
    "train_diffuse_nebula_names = os.listdir(train_diffuse_nebula_dir)\n",
    "train_planetary_nebula_names = os.listdir(train_planetary_nebula_dir)\n",
    "train_spiral_galaxy_names = os.listdir(train_spiral_galaxy_dir)\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 8\n",
    "ncols = 8\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_dark_nebula_pix = [os.path.join(train_dark_nebula_dir, fname)\n",
    "                        for fname in train_dark_nebula_names[pic_index-8:pic_index]]\n",
    "next_diffuse_nebula_pix = [os.path.join(train_diffuse_nebula_dir, fname)\n",
    "                           for fname in train_diffuse_nebula_names[pic_index-8:pic_index]]\n",
    "next_planetary_nebula_pix = [os.path.join(train_planetary_nebula_dir, fname)\n",
    "                             for fname in train_planetary_nebula_names[pic_index-8:pic_index]]\n",
    "next_spiral_galaxy_pix = [os.path.join(train_spiral_galaxy_dir, fname)\n",
    "                          for fname in train_spiral_galaxy_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_dark_nebula_pix+next_planetary_nebula_pix+next_diffuse_nebula_pix+next_spiral_galaxy_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off')  # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  lum_img = img[:, :, 0]\n",
    "  plt.imshow(lum_img)\n",
    "  plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "x = layers.Flatten()(last_output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(11, activation='softmax')(x)\n",
    "model = Model(pre_trained_model.input, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    # steps_per_epoch = 100,\n",
    "    epochs=20,\n",
    "    # validation_steps = 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('GLIESE_22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9709c04f27a0412169b16356b03e46057bd7975a3ed563387551a23d041a241b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
